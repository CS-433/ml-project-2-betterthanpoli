{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN2EghZFWvuF",
        "outputId": "0ae38755-7a9d-499c-f8a2-0e54881fa6b6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "c93raXY3WRrz",
        "outputId": "5d9f48b0-9e8b-4c61-8cc8-eba54c5fc4fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:19, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#!pip install accelerate -U\n",
        "#!pip install transformers[torch]\n",
        "#! pip install -U accelerate\n",
        "#! pip install -U transformers\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Load pretrained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Load your custom dataset\n",
        "# Make sure your dataset is in a text format, where each line is a text sequence\n",
        "train_file = \"drive/MyDrive/MLProject2/hp.txt\"\n",
        "\n",
        "# Tokenize and prepare the dataset\n",
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=train_file,\n",
        "    block_size=128,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # If True, you're treating this as a masked language model task\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"drive/MyDrive/MLProject2\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Create Trainer and fine-tune the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "fine_tuned_model_path = \"drive/MyDrive/MLProject2\"  # Change this to the path where you saved your fine-tuned model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(fine_tuned_model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(fine_tuned_model_path)\n",
        "\n",
        "# Ask a question\n",
        "question = \"Do heat pumps help saving energy?\"\n",
        "\n",
        "# Tokenize the question\n",
        "input_ids = tokenizer.encode(question, return_tensors=\"pt\")\n",
        "\n",
        "# Generate a response\n",
        "output = model.generate(input_ids, max_length=200, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
        "\n",
        "# Decode and print the generated response\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Generated Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugxwmQ9EX8O6",
        "outputId": "6a0262e6-4c1d-4fbe-a2b2-c7e9eb625ec3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Response: Do heat pumps help saving energy?\n",
            "\n",
            "Heat pumps can help save energy by reducing the amount of heat that can be generated in a given area. Heat pumps are designed to reduce the heat generated by heat exchangers and other heat-generating equipment. They can also be used to heat up buildings, reduce heat loss to the environment, and reduce greenhouse gas emissions. For example, a heat pump can reduce energy consumption by up to 50% by using less heat, while also reducing energy use by as much as 20%. Heat pump systems can provide a range of energy saving applications, including heat storage, heat management, energy efficiency and energy conservation, as well as heat transfer and heat transport.\n",
            ".Heat pump system is an efficient, cost-effective and environmentally friendly way to store and transport heat. It is also an effective means of storing and transporting heat in the form of liquid or liquid-based fuels, such as natural gas, oil, coal, hydrocarbons and hydrofluoric acid\n"
          ]
        }
      ]
    }
  ]
}